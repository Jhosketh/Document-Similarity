1 + 2
34 * 3
x = 1
x
c(1,2,3)
install.packages()
help(c)
apropos("nova"
)
apropos("cat")
class(x)
is.integer(k)
is.integer(x)
y = as.integer(5)
class(y)
is.integer(y)
v = as.integer(4.53)
v
v = as.integer(4.99)
v
v = as.integer("444")
v
v = as.integer("miaw"
)
v = as.integer("miaw")
v
v = as.integer("TRUE")
v = as.integer(TRUE)
v
v = as.integer(FALSE)
v
v = as.integer(true)
z = 1 + 2i
z
class(z)
sqrt(-1(
f)
)
sqrt(-1)
sqrt(z)
sqrt(as.complex(-1))
s = 5 > 9
s
s = 5 >2
s
s = 5 & 3
s
s = TRUE & FALSE
s
s = TRUE & TRUE
s
s = false & true
help(
'&')
p = paste("kucing", as.character(terbang))
p = paste("kucing", as.character(5))
p = paste("kucing", as.character(5))
p = paste(p, " terbang")
p
help("sub")
s = c("aa", "bb", "cc", "dd", "ee") 
s
s[3]
s[2]
s[0]
s[1]
s[-1]
s[-5]
s[-2]
s[-0]
s[5]
s[6]
s[c(1,2)]
s[c(4,1)]
s[c(1,4)]
s[c(1:4)]
help(":")
l = c(TRUE, FALSE, TRUE)
l
s[L]
s[l]
v = c("Mary", "Sue")
v
names(v) = c("Kucing", "Terbang")
v
v[c("Terbang")]
v[c("Terbang", "Kucing", "Kucing")]
A = matrix( 
+   c(2, 4, 3, 1, 5, 7), # the data elements 
+   nrow=2,              # number of rows 
+   ncol=3,              # number of columns 
+   byrow = TRUE)        # fill matrix by rows 
A = matrix(
c(2, 4, 3, 1, 5, 7), # the data elements 
nrow=2,              # number of rows 
ncol=3,              # number of columns 
byrow = TRUE)        # fill matrix by rows 
a
A
A[2,1]
A[,2]
A[,c(1,3)]
dimnames(A) = list(
c("row1", "row2"),
c("col1", "col2", "col3"))
A
A
t(A)
q()
A
n = c(2, 3, 5) 
s = c("aa", "bb", "cc", "dd", "ee") 
b = c(TRUE, FALSE, TRUE, FALSE, FALSE)
x = list(n, s, b, 3)   # x contains copies of n, s, b
x
x[1]
x[2]
x[2,3]
x[2:3]
x[1:3}
x[1:3]
x[c(1,2)]
x
x[1][1] = 99
x
x[[2]][1]
x[[2]][1] = "miw"
x[[2]][1]
x
s
a
n
x[2][1] = "aa"
x
n
mtcars
help(mtcars)
head(mtcars)
mtcars
mtcars[[1]]
mtcars[1]
mtcars[,1]
mtcars[1,]
mtcars[am==0]
mtcars["am"==0]
mtcars[,"am"==0]
mtcars[,"am"]
mtcars$am
mtcars$am==0
mtcars[,"am"==0]
mtcars$am==0
mtcars[mtcars$am == 0,]
mtcars[mtcars$am == 0,]$cyl
mtcars[,"am"]==0
library(gdata)
install.packages
install.package("gdata")
install.packages("gdata")
library(gdata)
help(read.xls)
mydata = read.xls("mydata.xls")
getwd)_
getwd()
mydata = read.xls("titanic.xls")
utils:::menuInstallPkgs()
utils:::menuInstallLocal()
utils:::menuInstallLocal()
utils:::menuInstallLocal()
library(gdata)
help(read.xls)
mydata = read.xls("mydata.xls")
mydata = read.xls("mydata.xls")
q()
my.corpus
a
x
ls()
rm(ls())
rm(l = ls())
rm(list = ls())
ls()
unlink(".RData") 
Sys.setenv(NOAWT=TRUE)
my.corpus <- Corpus(DirSource("C:/Users/Tifani/Dropbox/Informatics - Computer Science/IF4090 - Industrial Practice/Projects/Document Similarity/dataset/corpus/r-corpus"))
require("tm")
save.image("C:\\Users\\Tifani\\Documents\\GitHub\\Document-Similarity\\Document Similarity\\.RData")
getwd()
my.corpus <- Corpus(DirSource("C:/Users/Tifani/Documents/GitHub/Document-Similarity/Document Similarity/corpus"))
rm(my.corpus)
my.corpus
CORPUS_PATH = "C:/Users/Tifani/Documents/GitHub/Document-Similarity/Document Similarity/corpus"
CORPUS_PATH
my.corpus <- Corpus(DirSource(path))
my.corpus <- Corpus(DirSource(CORPUS_PATH))
getTransformations
my.corpus
remove(my.corpus)
my.corpus
corpus <- Corpus(DirSource(CORPUS_PATH))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
require("Snowball")
require("SnowballC")
corpus <- tm_map(corpus, stemDocument)
save.image("C:\\Users\\Tifani\\Documents\\GitHub\\Document-Similarity\\Document Similarity\\.RData")
tdm <- TermDocumentMatrix(corpus)
tdm
inspect(tdm)
df <- as.data.frame(inspect(tdm))
CORPUS_TEST_PATH = "C:/Users/Tifani/Documents/GitHub/Document-Similarity/Document Similarity/corpus/test"
rm(corpus)
# Load Corpus to Workspace
corpus <- Corpus(DirSource(CORPUS_TEST_PATH))
# Cleaning Corpus (getTransformations for info)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus)
inspect(my.tdm)
inspect(tdm)
rm(corpus)
# Load Corpus to Workspace
corpus <- Corpus(DirSource(CORPUS_TEST_PATH))
# Cleaning Corpus (getTransformations for info)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(corpus)
inspect(tdm(
))
inspect(tdm)
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus)
dtm <- DocumentTermMatrix(corpus)
inspect(dtm)
inspect(dtm)
inspect(dtm)
rm(dtm)
dtm
rdm
tdm
inspect(tdm)
findFreqTerms(my.tdm, 2)
findFreqTerms(tdm, 2)
findAssocs(my.tdm, 'mine', 0.20)
findAssocs(tdm, 'mine', 0.20)
findAssocs(tdm, 'writer', 0.20)
df <- as.data.frame(inspect(tdm))
df <- as.data.frame(inspect(tdm))
df <- as.data.frame(inspect(tdm))
df.scale <- scale(df)
d <- dist(df.scale, method="euclidean")
fit <- hclust(d, method="ward")
fit <- hclust(d, method="ward.D2")
plot(fit)
q()
findFreqTerms(tdm, 6)
findFreqTerms(tdm, 9)
findFreqTerms(tdm, 9)
inspect(tdm)
df <- as.data.frame(inspect(tdm))
df.scale <- scale(df)
d <- dist(df.scale, method="euclidean")
fit <- hclust(d, method="ward.D2")
df <- as.data.frame(inspect(tdm))
df.scale <- scale(df)
d <- dist(df.scale, method="euclidean")
fit <- hclust(d, method="ward.D2")
plot(fit)
plot(fit)
dtm <- DocumentTermMatrix(corpus)   
freq <- colSums(as.matrix(dtm))   
length(freq)   
str(freq)
freq
freq <- sort(frequency, decreasing=TRUE)  
freq <- sort(freq, decreasing=TRUE)  
freq
library("wordcloud")
install.packages("wordcloud")
library("wordcloud")
word <- names(freq)
wordcloud(words[1:100], freq[1:100])
rm(word)
words <- names(freq)
wordcloud(words[1:100], freq[1:100])
m <- as.matrix(dtm)   
dim(m)   
write.csv(m, file="dtm.csv")
setwd("C:/Users/Tifani/Documents/GitHub/Document-Similarity/Document Similarity/corpus"
getwd
getwd()
setwd("C:/Users/Tifani/Documents/GitHub/Document-Similarity/Document Similarity/")
getwd()
m <- as.matrix(dtm)   
dim(m)   
write.csv(m, file="dtm.csv") 
corpus <- Corpus(DirSource(CORPUS_PATH))
corpus <- Corpus(DirSource(CORPUS_TEST_PATH))
corpus <- Corpus(DirSource(CORPUS_TEST_PATH))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
dtm <- DocumentTermMatrix(corpus)   
m <- as.matrix(dtm)   
dim(m)   
write.csv(m, file="dtm.csv")
dtms <- removeSparseTerms(dtm, 0.1)
dtms
inspect(dtms)  
dtm <- DocumentTermMatrix(corpus)   
dtms <- removeSparseTerms(dtm, 0.1)  
dtms
freq <- colSums(as.matrix(dtm))   
length(freq)   
ord <- order(freq)   
freq[head(ord)]   
ord
inspect(dtm)
freq <- colSums(as.matrix(dtm))   
freq
ord <- order(freq)
ord
freq[head(ord)]
head(table(freq), 20)   
tail(table(freq), 20)   
freq <- colSums(as.matrix(dtms))   
freq
corpus <- Corpus(DirSource(CORPUS_TEST_PATH))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus)
dtm <- DocumentTermMatrix(corpus) 
freq <- colSums(as.matrix(dtm))   
length(freq)
freq
ord <- order(freq)
ord
freq[head(ord)]
dtm
inspect(dtm)
head(table(freq), 20)   
freq <- colSums(as.matrix(dtms))   
freq
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)   
head(freq, 14)   
dtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.   
dtms
inspect(dtms)  
freq <- colSums(as.matrix(dtms))   
freq
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)   
head(freq, 14)
findFreqTerms(dtm, lowfreq=50)   # Change "50" to whatever is most appropriate for your text data.
wf <- data.frame(word=names(freq), freq=freq)   
head(wf)  
library(ggplot2)   
p <- ggplot(subset(wf, freq>50), aes(word, freq))    
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
install.packages(ggplot2)
install.packages("ggplot2")
library(ggplot2)   
p <- ggplot(subset(wf, freq>50), aes(word, freq))    
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
p   
Sys.setenv(NOAWT=TRUE)
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc")
install.packages(Needed, dependencies=TRUE)   
Needed <- c("tm", "SnowballC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc")
install.packages(Needed, dependencies=TRUE)   
require("tm")
require("SnowballC")
library("wordcloud")
corpus <- Corpus(DirSource(CORPUS_TEST_PATH))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus)
library(slam)
cosine_dist_mat <- 1 - crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
cosine_dist_mat
corpus <- Corpus(DirSource(CORPUS_TEST_PATH))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
tdm <- TermDocumentMatrix(corpus)
cosine_dist_mat <- 1 - crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
cosine_dist_mat
tdm
inspect(tdm)
inspect(tdm)
cosine_dist_mat
corpus <- Corpus(DirSource(CORPUS_TEST_PATH))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
tdm <- TermDocumentMatrix(corpus)
tdm
inspect(tdm)
cosine_dist_mat <- 1 - crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
cosine_dist_mat
cosine_dist_mat <- crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
cosine_dist_mat
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus)
cosine_dist_mat <- crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
cosine_dist_mat
require("tm")
require("SnowballC")
library(slam)
corpus <- Corpus(DirSource(CORPUS_TEST_PATH))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus)
cosine_dist_mat <- crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
m <- as.matrix(cosine_dist_mat)   
dim(cosine_dist_mat)   
write.csv(m, file="cosine_dist_mat.csv") 
cosine_dist_mat
corpus <- Corpus(DirSource(CORPUS_TEST_PATH))
require("tm")
require("SnowballC")
library(slam)
corpus <- Corpus(DirSource(CORPUS_TEST_PATH))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus)
cosine_dist_mat <- crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
m <- as.matrix(cosine_dist_mat)   
dim(cosine_dist_mat)   
write.csv(m, file="cosine_dist_mat.csv") 
cosine_dist_mat
cosine(tdm, NULL)
require("lsa")
install.packages("lsa")
require("lsa")
cosine(tdm, NULL)
cosine(tdm, y = NULL)
tdm
mat-tdm <- as.matrix(tdm)
mattdm <- as.matrix(tdm)
mattdm
head(mattdm)
mattdm <- as.data.frame(inspect(tdm))
head(mattdm)
cosine(mattdm, y = NULL)
corpus <- Corpus(DirSource(CORPUS_TEST_PATH))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus)
cosine_dist_mat <- crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2)
)
))
cosine_dist_mat <- crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
cosine_dist_mat
tdm
mattdm <- as.matrix(tdm)
mattdm
cosine(mattdm, y = NULL)
cosine_dist_mat
mattdm <- as.matrix(tdm)
corpus <- Corpus(DirSource(CORPUS_TEST_PATH))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus)
cosine_dist_mat_v1 <- crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
m <- as.matrix(cosine_dist_mat_v1)   
dim(cosine_dist_mat_v1)   
write.csv(m, file="cosine_dist_mat_v1.csv") 
mattdm <- as.matrix(tdm)
cosine_dist_mat_v2 <- cosine(mattdm, y = NULL)
m <- as.matrix(cosine_dist_mat_v2)   
dim(cosine_dist_mat_v2)   
write.csv(m, file="cosine_dist_mat_v2.csv") 
cosine_dist_mat_v1
cosine_dist_mat_v1
cosine_dist_mat_v2
tdm
inspect(tdm)
tdm
inspect(tdm)
weightBin(tdm)
inspect(tdm)
cosine_dist_mat_v1 <- crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
q()
